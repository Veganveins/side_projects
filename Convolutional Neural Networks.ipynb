{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole idea behind a \"neural network\" is to imitate how humans learn. So how do humans learn?\n",
    "\n",
    "A neurons collects signals from input channels named \"dendrites,\" processes information in its nucleus, and then generates an output in a long thin branch called an axon. Humans learn as they change the strength of the bonds between neurons. A neural network is an attempt to represent this all mathematically.\n",
    "\n",
    "We can represent the biological nucleus of a neuron as a summation. We can think of the summation as the total of all of the inputs (training data) multiplied by \"weights\" -- essentially representing the strength of the bonds between neurons in a human brain. Simply stated, larger sums equate to stronger bonds. Traditional neural networks also include a bias term that can shift the summation value up or down. The summation is then filtered through what is called an activation function (common ones include sigmoid (binary classification), softmax (multi-class classification), relu, etc). This activation function is kind of the crux of deep learning; without it, the weights and bias would simply do a linear transformation (essentially just a linear regression model). The activation function allows the network to learn complex non-linear patterns. Understanding which activation function to use is a critical step in designing any neural network.\n",
    "\n",
    "Imagine taking thousands of these \"artifical neurons\" connected together. This is a neural network in something called a \"hidden layer.\" The hidden layer is connected to an output layer which tells the network what it should predict for a given set of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk a little bit more about the inputs. How could a neural netowrk take for example an image and generate a prediction for the type of article of clothing in the image? You cannot just take an image and feed it directly to a network in the same way you could hold up a picture to a human person. The pipeline from image to network input requires some legwork and goes something like this: take an image, say for example of a shoe. The shoe image undergoes a process called \"convolution\" (to be defined later), \"pooling\" (essentially downsampling), and \"flattening\" before it is ready to be fed into a CNN as an input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutions use a kernel matrix to scan a given image and apply a filter to obtain a certain effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An image kernel is a matrix used to apply effects such as blurring and sharpening. They are used in ML for feature extraction to select the _most important_ pixels of an image. Convolutions preserve the spatial relationship between pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature maps: outputs of convolution (the process of running a feature detector on an actual image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature map will be the same size as the feature detector, the feature map will be a new version of the image over which the feature detector was applied. The feature map might be a blurred version, a sharpened version, etc depending on the values in the feature detector used to multiply the pixel values in your image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pooling (downsampling layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helps avoid overfitting by reducing feature map dimensionality. This improves computational efficiency while preserving the features. Max pooling works by returning the maximum feature response within a given sample size in a feature map. (Min, avg pooling etc also exist). This allows us to move from say 40x40 to 20x20. We keep the prominent features but represent it in a much more condensed form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flattening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts a 2x2 for example into a vector that can be used as an input to a CNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "train = pd.read_csv('fashion-mnist_train.csv')\n",
    "test = pd.read_csv('fashion-mnist_test.csv')\n",
    "\n",
    "training = np.array(train, dtype = 'float32')\n",
    "testing = np.array(test, dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 785)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEP5JREFUeJzt3V9snfV9x/HP1/bxn9iOm5ACaRL+lYwO6AatB92YKlZGR6dqwEVRc1FlUtX0omir1IshbsouJqFpbcfFVCmMqEHqX6llcIE2EPvDUDdEYIw/S1tYCOAmixOSNIkT28fH3134pDLg5/s4Pn+d7/sloRyf33n8fH3w5zzn+Pv8np+5uwDk09PpAgB0BuEHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5BUXzt31m8DPqjhdu5yVZjZFD8n/SfjszDtxOlmltM91gyGw7NjveF4/8GpZlazKkxrSrM+Y8t5bEPhN7PbJD0gqVfS37v7/dHjBzWsG+2WRnZ5Xtr3Z78bjm/552o4XnliTzPLebeeOGCar7Vs1/aRa8LxN/9kLBy/5C9/2sxyVoVn/allP3bFb/vNrFfS30n6jKSrJW0zs6tX+v0AtFcjn/lvkPS6u+9z91lJP5B0e3PKAtBqjYR/k6S3F309Ub/vXcxsh5ntMbM9Vc00sDsAzdRI+Jf6o8L7/jLl7jvdfdzdxysaaGB3AJqpkfBPSNqy6OvNkg40Vg6Admkk/M9J2mpml5tZv6TPS3qsOWUBaLUVt/rcfc7M7pb0T1po9e1y91ebVlkiY/8bjw/836lwvOfySwvHvC9u1dVe2xfvvMFWXt+lWwrHfCo+P8Emj4Xj6/eOrqgmLGioz+/uj0t6vEm1AGgjTu8FkiL8QFKEH0iK8ANJEX4gKcIPJNXW+fxY2vpX43734RvXheMXPPizwrHedfG2UR9ekuaPHg/HbaA/3v7wO8Vjp+Of+/SdN4bjwxNnwnHEOPIDSRF+ICnCDyRF+IGkCD+QFOEHkqLV1wXsP18Jx6u/E7e85j718eLBf30x3vmxeNps7zVXheO1va/H3z+aEvyJ34o3rcRXoLaf/ne8b4Q48gNJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUvT5u8D8TXG/e/YD8fZHPlq8EtL8eHyOwMZn4mWsayW9dLs+Xkn34CeLV9KdL1nAafSt+XC89ByEV38e7yA5jvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kFRDfX4z2y/ppKSapDl3H29GUdlMfGooHJ8b8nB87PXi8b7peNvq2vjS25Xf++1wfGrjYDi+4eWZwrGBQ/HS4+98bH04/qtr4suSj7BgfKgZJ/n8gbsfacL3AdBGvO0Hkmo0/C7pCTN73sx2NKMgAO3R6Nv+m9z9gJldKOlJM/uZuz+9+AH1F4UdkjSoNQ3uDkCzNHTkd/cD9X8nJT0i6YYlHrPT3cfdfbyikpkcANpmxeE3s2EzGz17W9KnJcWXoQXQNRp523+RpEfM7Oz3+Z67/2NTqgLQcisOv7vvkxQ3gbEsG14Krm0v6cQlveF4T624l2/xlHgdv7ISjs8NxuP9J+PzCKLta0Nrw21PXhpft3/tG/G+EaPVByRF+IGkCD+QFOEHkiL8QFKEH0iKS3d3geH98dTWX/7hSDg+dKS4JXZmU9wmPH1x3C6rnIrbbb3FM3YlSTNjxceXMxviY0/to/Hzsv6ReOdxAxUc+YGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKfr8XWDij4qXsZakyoa4391bLb482nzc5tfg4biPXx2Nt6+uibefD2YE95Q04sdGzoTjZ7bEa5f3c+nuEEd+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iKPn8X8JKX4Op0/L9pZqy41z4bt8I1X2ns8tdzI3GfP1Ir+7ln4uXDq1fElxW/8FwLSoYjP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kVdrnN7Ndkj4radLdr63ft17SDyVdJmm/pLvc/Vjryjy/RXPeJcmr8Wt0NKe+rI9fdo5BT7Wkj1+yBHh0eKmOlCzvPRX3+Wcuibenzx9bzpH/O5Jue89990h6yt23Snqq/jWAVaQ0/O7+tKSj77n7dkm767d3S7qjyXUBaLGVfua/yN0PSlL9X95hAatMy8/tN7MdknZI0qCKrzUHoL1WeuQ/ZGYbJan+72TRA919p7uPu/t4RQMr3B2AZltp+B+TtL1+e7ukR5tTDoB2KQ2/mX1f0n9IusrMJszsi5Lul3Srmb0m6db61wBWkdLP/O6+rWDolibXkpaVLSRf8hI9H3yamo9b5bK5kvGSPn7p9w+2n91Q8oPPxIsOzK8rKR4hzvADkiL8QFKEH0iK8ANJEX4gKcIPJMWlu7tAbTCemto7GLe05oaK+21za+N2mpVM2a2VLME9OBkfP2pBG7J/3XS47ezx+IzQnjMl648jxJEfSIrwA0kRfiApwg8kRfiBpAg/kBThB5Kiz78K9Fh8HkA0Jbisj+/98Zxd7y259PeReE5vVFtvb7zvytrZcHzgteFwHDGO/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFH3+LlDdMhOOV3riXnt1bTC+thpuu3bsTDh+4mjcSy9bZrs2WtzLr9TiY8+mDcfD8aNV+vyN4MgPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0mV9vnNbJekz0qadPdr6/fdJ+lLkg7XH3avuz/eqiLPd1s3T4bjb0xeEI5H1+bvL7nm/+Xrjobjv5iLr40/cyL+FfK+4j5/raTPf8HgVDg+sSk+xwCx5Rz5vyPptiXu/5a7X1f/j+ADq0xp+N39aUnx4QHAqtPIZ/67zewlM9tlZuuaVhGAtlhp+L8t6cOSrpN0UNI3ih5oZjvMbI+Z7akqPocdQPusKPzufsjda+4+L+lBSTcEj93p7uPuPl5RvPAigPZZUfjNbOOiL++U9EpzygHQLstp9X1f0s2SNpjZhKSvS7rZzK6T5JL2S/pyC2sE0AKl4Xf3bUvc/VALajlv9W36UDged+LL9QwXz9n3+fi6/X09wYX1JfWUXEvAK/F4z5rin642Hf/6nZ6L1wS44uNvh+OIcYYfkBThB5Ii/EBShB9IivADSRF+ICku3d0G01dtDMfn54+E40ND8VLVU6eLz5zsH4gbiR8ZPRSOv3UinrYxFUzZlaS+YHxgKL6s+FvH4n2vGYiflw0biqdC1468E26bAUd+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iKPn8bnNoUT039QMm02rGh6XA8XEZ7MO6lXzs0EY4/Xrs6HFfJlF4LpgRX+uKfe3qmEo4Pj8Z9/rnf2FxcF31+jvxAVoQfSIrwA0kRfiApwg8kRfiBpAg/kBR9/jaYHY0vnz3aH/fxe3viOfM9leJ+edm2J+aHwvHafHx8qJTMye/tDZboLvneZa5cG18HYe/GiwvHgjMj0uDIDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJlfb5zWyLpIclXSxpXtJOd3/AzNZL+qGkyyTtl3SXux9rXamr19TmeM57o3qja+NX4uv2X1Y53NC+y9YFmA+WCI/GpPgcAUmaKlnCu7qGY1tkOc/OnKSvuftvSvqEpK+Y2dWS7pH0lLtvlfRU/WsAq0Rp+N39oLu/UL99UtJeSZsk3S5pd/1huyXd0aoiATTfOb0vMrPLJF0v6VlJF7n7QWnhBULShc0uDkDrLDv8ZjYi6ceSvuruJ85hux1mtsfM9lQ1s5IaAbTAssJvZhUtBP+77v6T+t2HzGxjfXyjpMmltnX3ne4+7u7jFRUvKAmgvUrDb2Ym6SFJe939m4uGHpO0vX57u6RHm18egFZZzpTemyR9QdLLZvZi/b57Jd0v6Udm9kVJb0n6XGtKXP2q6+NLVJ8uaVmVTX2d9+KW2Vwt3vbC3lPheNnlswcH4im91Wq8faQSTFWWpOOzJdORB1e86xRKw+/uz0gq+u26pbnlAGgXzoIAkiL8QFKEH0iK8ANJEX4gKcIPJMWlu9ugZzp+je0vWaL7V3Nxr9xrxX3+sim9H+qLx71s2m3JpcGjabmzs/GvX9mU3mPTcZ+/OhLXnh1HfiApwg8kRfiBpAg/kBThB5Ii/EBShB9Iij5/G4y8Gb/GHti6Nhwvm5M/X+0tHKuU9OErhbO1F1TPxOcYzJZcunsuqC06P0GSaiU/90yVX99GcOQHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaRolLZBT8kqZafOxCsZjQyVfIPqyuetD1jJr0B8mkDpMtvRmgLR+QmSVOttbGnzsuc9O478QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5BUaZ/fzLZIeljSxVro+u509wfM7D5JX5J0uP7Qe9398VYVupoNHY2b5SdLrl8/0xdf199mi1/DN48cD7dd09Mfjqtkzn2Zsjn74a6n4+dlqjc+P+KDh+LnLbvlnOQzJ+lr7v6CmY1Ket7MnqyPfcvd/6Z15QFoldLwu/tBSQfrt0+a2V5Jm1pdGIDWOqfP/GZ2maTrJT1bv+tuM3vJzHaZ2bqCbXaY2R4z21MV51sC3WLZ4TezEUk/lvRVdz8h6duSPizpOi28M/jGUtu5+053H3f38Yriz2gA2mdZ4TezihaC/113/4kkufshd6+5+7ykByXd0LoyATRbafjNzCQ9JGmvu39z0f0bFz3sTkmvNL88AK2ynL/23yTpC5JeNrMX6/fdK2mbmV0nySXtl/TlllR4HhiarIbjw2viv4WMDU2H46dqY4Vj144eCLetekk7rGRabU9PPD4yWlz79Ex8WfAyc7PxlOC+6ZL5yMkt56/9z0hLXtydnj6winGGH5AU4QeSIvxAUoQfSIrwA0kRfiApLt3dBgP/tS8cP3HoynB8ajQ+Lbr/WPFr+L8d3hpu+8aZDeH4mn3xlN+pufj4UTkaLNFdcuipDcd9+jWbToXj3tPYeQTnO478QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5CUuTe2DPI57czssKQ3F921QdKRthVwbrq1tm6tS6K2lWpmbZe6+weX88C2hv99Ozfb4+7jHSsg0K21dWtdErWtVKdq420/kBThB5LqdPh3dnj/kW6trVvrkqhtpTpSW0c/8wPonE4f+QF0SEfCb2a3mdnPzex1M7unEzUUMbP9Zvaymb1oZns6XMsuM5s0s1cW3bfezJ40s9fq/y65TFqHarvPzH5Zf+5eNLM/7lBtW8zsX8xsr5m9amZ/Xr+/o89dUFdHnre2v+03s15Jv5B0q6QJSc9J2ubu/9PWQgqY2X5J4+7e8Z6wmX1S0ilJD7v7tfX7/lrSUXe/v/7Cuc7d/6JLartP0qlOr9xcX1Bm4+KVpSXdIelP1cHnLqjrLnXgeevEkf8GSa+7+z53n5X0A0m3d6COrufuT0s6+p67b5e0u357txZ+edquoLau4O4H3f2F+u2Tks6uLN3R5y6oqyM6Ef5Nkt5e9PWEumvJb5f0hJk9b2Y7Ol3MEi6qL5t+dvn0Cztcz3uVrtzcTu9ZWbprnruVrHjdbJ0I/1Kr/3RTy+Emd/+YpM9I+kr97S2WZ1krN7fLEitLd4WVrnjdbJ0I/4SkLYu+3iwpXlCujdz9QP3fSUmPqPtWHz50dpHU+r+THa7n17pp5ealVpZWFzx33bTidSfC/5ykrWZ2uZn1S/q8pMc6UMf7mNlw/Q8xMrNhSZ9W960+/Jik7fXb2yU92sFa3qVbVm4uWllaHX7uum3F646c5FNvZfytpF5Ju9z9r9pexBLM7AotHO2lhSsbf6+TtZnZ9yXdrIVZX4ckfV3SP0j6kaRLJL0l6XPu3vY/vBXUdrMW3rr+euXms5+x21zb70v6d0kvSzp7CeB7tfD5umPPXVDXNnXgeeMMPyApzvADkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5DU/wNH4dUBS7cXmwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "i = random.randint(1,60000)\n",
    "plt.imshow(training[i, 1:].reshape(28,28))\n",
    "label = training[i, 0]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>103</td>\n",
       "      <td>87</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>53</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>53</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>137</td>\n",
       "      <td>126</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>224</td>\n",
       "      <td>222</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      0       0       0       0       0       0       0       0       9   \n",
       "1      1       0       0       0       0       0       0       0       0   \n",
       "2      2       0       0       0       0       0       0      14      53   \n",
       "3      2       0       0       0       0       0       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0       8  ...       103        87        56         0         0         0   \n",
       "1       0  ...        34         0         0         0         0         0   \n",
       "2      99  ...         0         0         0         0        63        53   \n",
       "3       0  ...       137       126       140         0       133       224   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2        31         0         0         0  \n",
       "3       222        56         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training[:, 1:]/255\n",
    "y_train = training[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = testing[:, 1:]/255\n",
    "y_test = testing[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size = .2, random_state = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], *(28, 28, 1))\n",
    "X_test = X_test.reshape(X_test.shape[0], *(28, 28, 1))\n",
    "X_validate = X_validate.reshape(X_validate.shape[0], *(28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jj5jxt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(28, 28, 1..., activation=\"relu\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "cnn_model.add(Conv2D(32, 3, 3, input_shape = (28, 28, 1), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.add(MaxPooling2D(pool_size = (2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jj5jxt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=32)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "cnn_model.add(Dense(output_dim = 32, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jj5jxt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=10)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "cnn_model.add(Dense(output_dim = 10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(loss = 'sparse_categorical_crossentropy', optimizer = Adam(lr = 0.001), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jj5jxt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 5s 95us/step - loss: 0.7963 - acc: 0.7381 - val_loss: 0.4930 - val_acc: 0.8293\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.4420 - acc: 0.8469 - val_loss: 0.4069 - val_acc: 0.8577\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.3858 - acc: 0.8638 - val_loss: 0.3734 - val_acc: 0.8710\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 4s 92us/step - loss: 0.3538 - acc: 0.8767 - val_loss: 0.3423 - val_acc: 0.8842\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 4s 90us/step - loss: 0.3292 - acc: 0.8860 - val_loss: 0.3245 - val_acc: 0.8848\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.3137 - acc: 0.8912 - val_loss: 0.3209 - val_acc: 0.8869\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.2985 - acc: 0.8964 - val_loss: 0.3084 - val_acc: 0.8920\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 4s 92us/step - loss: 0.2883 - acc: 0.8986 - val_loss: 0.2987 - val_acc: 0.8958\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.2737 - acc: 0.9040 - val_loss: 0.2905 - val_acc: 0.8977\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.2663 - acc: 0.9067 - val_loss: 0.2832 - val_acc: 0.9006\n",
      "Epoch 11/50\n",
      "22016/48000 [============>.................] - ETA: 2s - loss: 0.2518 - acc: 0.9097"
     ]
    }
   ],
   "source": [
    "cnn_model.fit(X_train, y_train,\n",
    "             batch_size = 512,\n",
    "             nb_epoch = epochs,\n",
    "             verbose = 1, \n",
    "             validation_data = (X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = cnn_model.evaluate(X_test, y_test)\n",
    "evaluation[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key point is during training we reached about 95% accuracy. During testing we receive 91% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_classes = cnn_model.predict_classes(X_test)\n",
    "predict_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print a grid that will tell us here is the image with the predicted and actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 5\n",
    "W = 5\n",
    "fig, axes = plt.subplots(L, W, figsize = (12,12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in np.arange(0, L*W):\n",
    "    axes[i].imshow(X_test[i].reshape(28,28))\n",
    "    axes[i].set_title(\"Prediction Class = {:0.1f}\\n True Class = {:0.1f}\".format(predict_classes[i], y_test[i]))\n",
    "    axes[i].axis('off')\n",
    "    \n",
    "plt.subplots_adjust(wspace=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, predict_classes)\n",
    "plt.figure(figsize = (14,10))\n",
    "sns.heatmap(cm, annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gives an overview of how many classes have been classified correctly vs incorrectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "num_classes = 10\n",
    "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
    "\n",
    "print(classification_report(y_test, predict_classes, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision and recall are interesting concepts. My preferred way to remember which is which is with this story:\n",
    "\n",
    "Suppose you are in a room with a giant bowl full of skittles, regular m&m's, and peanut m&m's. Also suppose you have a nephew coming over who loves candy but who also is highly allergic to peanuts. You decide to build a model that can classify whether or not a given sample (or candy) is a skittle, an m&m, or a peanut m&m. \n",
    "\n",
    "Precision is a measure of how often the model is correct when it predicts the positive class. That is, of the times the model said \"that's a peanut!\", how many of those times was it actually a peanut m&m. \n",
    "\n",
    "Recall is a measure of _comprehensiveness_. That is, of the peanut m&m's in the bowl, how many did the model identify? A naive model could in theory say \"that's a peanut\" on every sample. In that case the boy-who-cried-wolfness of the model would skyrocket, but you could be sure that you would find all of the peanut m&m's. This wouldn't be super useful if you were trying to give your nephew some peanut m&ms.\n",
    "\n",
    "A good data scientist can leverage precision and recall scores to find a model that doesn't \"cheat.\" That is, if I try to optimize only for high precision, my model will get more conservative and my recall score will fall. If I try to optimize only for high recall, my model will devolve into a boy-who-cried-peanut and the precision score will plummet. By taking both metrics into account we obtain the f1 score, which is the harmonic mean of precision and recall. In certain cases recall can be weighted more heavily than precision or vice-versa, which may or may not make sense depending on your use case. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding dropout (regularization to avoid overfitting); expanding the size of the kernel from 32 to maybe 64; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout will typically decrease the training accuracy but marginally increase the testing accuracy. Feature engineering for texture, fabric, shape, and style variables, length, color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
